{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSVEP Decoding Framework\n",
    "\n",
    "I want to build a general pipeline for SSVEP EEG signal decoding, in which it is easy to complete basic EEG processing stages like:\n",
    "\n",
    "* cutting and slicing your data\n",
    "* filtering data\n",
    "* applying feature extraction method\n",
    "* matching pattern and get result\n",
    "\n",
    "The key inspiration of the framework design ethic is modular, and I want to take the manually parts like experimental information, special filters and feature extraction methods out of the main executing. So that it's much easier to re-write or add new minds into the framework. In the other words, it is a framework that flexible, easy for new learner and try something new.\n",
    "\n",
    "There are 3 main class defined in this framework. If you just focus on get model for online experiment or do some cross validation, the `data_runner` class and `data_cross_validation` class are what you need, you can just read through and run them. In the `filter_apply` class, you can configure your own time-filter parameters, and what I must admit is that the filter parameters in the current version is not the best, and haven't been optimized at all! \n",
    "\n",
    "You may notice that there are several functions above the main classes, they can be named as helper functions. I can move them together and separate them to a helper collection and make the main program clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary external packages here\n",
    "\n",
    "In this framework, I try to use external packages as little as possible. Compared to use the toolbox like mne, it's a bit of troublesome, but not too much. Jump out of the mne processing and data framework can make you understand the data route more clearly.\n",
    "\n",
    "However, if you like, you can feel free add some packages for boosting the function of the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trca_matrix(X):\n",
    "    \"\"\" TRCA kernel function\n",
    "\n",
    "    Args:\n",
    "        X (Numpy array): Three dimsional ndarry matrix, shape as (n_trials, n_channels, n_samples)\n",
    "\n",
    "    Returns:\n",
    "        spatial_filter(Numpy array): A ndarray vector, shape as (n_channels, )\n",
    "    \n",
    "    REF:\n",
    "    [1] M. Nakanishi, Y. Wang, X. Chen, Y. -T. Wang, X. Gao and T. -P. Jung, \"Enhancing Detection of SSVEPs for a High-Speed Brain Speller Using Task-Related Component Analysis,\" in IEEE Transactions on Biomedical Engineering, vol. 65, no. 1, pp. 104-112, Jan. 2018, doi: 10.1109/TBME.2017.2694818.\n",
    "    \"\"\"\n",
    "    n_chans = X.shape[1]\n",
    "    n_trial = X.shape[0]\n",
    "    S = np.zeros((n_chans, n_chans))\n",
    "    # Computation of correlation matrices\n",
    "    for trial_i in range(n_trial):\n",
    "        for trial_j in range(n_trial):\n",
    "            x_i = X[trial_i, :, :]\n",
    "            x_j = X[trial_j, :, :]\n",
    "            S = S + np.dot(x_i, x_j.T)\n",
    "    X = np.transpose(X, (1, 2, 0))\n",
    "    X1 = X.reshape((n_chans, -1),order='F')\n",
    "    X1 = X1 - np.mean(X1, axis=1, keepdims=True)\n",
    "    Q = np.dot(X1, X1.T)\n",
    "    S = np.matrix(S)\n",
    "    Q = np.matrix(Q)\n",
    "    # TRCA eigenvalue algorithm\n",
    "    [W, V] = np.linalg.eig(np.dot(Q.I,S))\n",
    "\n",
    "    spatial_filter = V[:, 0].reshape(-1)\n",
    "\n",
    "    return spatial_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(split_type='K_Fold',test_sample_num = None, total_sample_num = None):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        split_type (str, optional): Key words of the cross_validation method. Defaults to 'K_Fold'.\n",
    "        test_sample_num (int, optional): the number of test samples (trials) contained in one cross validatioon seperation. Defaults to None.\n",
    "        total_sample_num (int, optional): total samples (trials) of your dataset. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        Exception: The sample cannot be divided into test subsets\n",
    "        Exception: Split_type was not defined, use 'K_Fold' or define it manually\n",
    "\n",
    "    Returns:\n",
    "        numpy array: A matrix shape as (total_sample_num/test_sample_num, total_sample_num)\n",
    "    \"\"\"\n",
    "    if split_type == \"K_Fold\":\n",
    "        if total_sample_num%test_sample_num != 0:\n",
    "            raise Exception('The sample cannot be divided into test subsets')\n",
    "        else:\n",
    "            folders_num = int(total_sample_num/test_sample_num)\n",
    "            split_index = np.zeros((folders_num,total_sample_num))\n",
    "            for folder_index in range(folders_num):\n",
    "                split_index[folder_index, folder_index*test_sample_num:(folder_index+1)*test_sample_num] = np.ones((test_sample_num,))\n",
    "        return split_index\n",
    "    \n",
    "    raise Exception('split_type was not defined, use \\'K_Fold \\' or define it manually')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for train_test_split()\n",
    "split_index = train_test_split(test_sample_num=5,total_sample_num=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(testsample,spatial_filters,template_storage):\n",
    "    \"\"\" \n",
    "    Function for pattern matching, input a single trial test sample, then filtering with the pre-trained spatial filter, finally calculated person correlation with all the template signal(using np.corrcoef method)\n",
    "\n",
    "    Args:\n",
    "        testsample (Numpy array): A numpy array of single test sample, shape as (N_chan,N_samples)\n",
    "        spatial_filters (dict): spatial filter dictionary storage, the keys are the trial indexes, values are corresponding spatial filters which trained from feature extracted method \n",
    "        template_storage (dict): template dictionary storage, the keys are the trial indexes, values are corrsponding spatial filters which calculated from train samples.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Testsample should be a two dimensional vector\n",
    "\n",
    "    Returns:\n",
    "        Numpy array: correlation coefficient storage vector, shape as (N_trials,1) \n",
    "    \"\"\"\n",
    "    if len(testsample.shape) != 2:\n",
    "        raise TypeError('Testsample should be a two dimensional vector')\n",
    "    corrcoef_storage = list()\n",
    "    spatial_filter = np.squeeze(np.array(list(spatial_filters.values())))\n",
    "    for template_iter in template_storage.keys():\n",
    "        corrcoef_storage.append(np.corrcoef(np.dot(spatial_filter,testsample).reshape(1,-1),np.dot(spatial_filter,template_storage[template_iter]).reshape(1,-1))[0,1])\n",
    "    \n",
    "    corrcoef_storage = np.array(corrcoef_storage)\n",
    "\n",
    "    return corrcoef_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_applyer():\n",
    "    \"\"\"\n",
    "    A filter warp class, you can design and build time-filter by modify or inherit and rewrite the _filter_design method. With this class, you can easily try different filter paramters setting without break the structure.\n",
    "\n",
    "    Attention, this filter_appler only support band-pass filter paramter, for it is the most common type in EEG processing.\n",
    "    \"\"\"\n",
    "    def __init__(self,sample_rate,high_cut_frequency,low_cut_frequency,filter_order,filter_type = 'FIR') -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sample_rate (float): Sample rate of filter\n",
    "            high_cut_frequency (float): High cut-off frequence of the band-pass filter\n",
    "            low_cut_frequency (float): Low cut-off frequence of the band-pass filter\n",
    "            filter_order (int): Filter order of FIR filter\n",
    "            filter_type (str,option): Choose a filter type, in this veision only support FIR which is also default.\n",
    "        \"\"\"\n",
    "        self.sample_rate = sample_rate\n",
    "        self.filter_type = filter_type\n",
    "        self.high_cut_frequency = high_cut_frequency\n",
    "        self.low_cut_frequency = low_cut_frequency\n",
    "        self.filter_order = filter_order\n",
    "        self._filter_design()\n",
    "    \n",
    "    def _filter_design(self):\n",
    "        if self.filter_type == 'FIR':\n",
    "            if self.filter_order%2 == 0:\n",
    "                self.filter_order+=1\n",
    "            # Nyquist rate of signal\n",
    "            nyq_rate = self.sample_rate/2.0\n",
    "            # 1-D array cut of\n",
    "            freq_cutoff = [self.high_cut_frequency/nyq_rate, self.low_cut_frequency/nyq_rate]\n",
    "            # Get the fir filter coef\n",
    "            taps = scipy.signal.firwin(self.filter_order, freq_cutoff, window='hamming', pass_zero='bandpass')\n",
    "            self.filter_b = taps\n",
    "            self.filter_a = 1\n",
    "        elif self.filter_type == 'IIR':\n",
    "            pass\n",
    "            \n",
    "            \n",
    "    def filter_apply(self,data):\n",
    "        filter_data = scipy.signal.lfilter(self.filter_b,self.filter_a,data)\n",
    "        return filter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test filter\n",
    "filter = filter_applyer(250,6,80,48,'FIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class result_analyser():\n",
    "    def __init__(self,labels,CV_loops,epoch_num) -> None:\n",
    "        self.labels = list(labels)\n",
    "        self.epoch_num = epoch_num\n",
    "        self.CV_loops = CV_loops\n",
    "        self.acc_storage = np.zeros(self.CV_loops)\n",
    "        self.result_matrix = np.zeros((CV_loops,len(self.labels),epoch_num))\n",
    "        self.trial_counter = 0\n",
    "    \n",
    "    def result_decide(self,coef_vector,CV_iter,trial_iter,epoch_iter):\n",
    "        _result = self.labels[np.argmax(coef_vector)]\n",
    "        self.trial_counter += 1\n",
    "        if _result == trial_iter:\n",
    "            self.result_matrix[CV_iter, self.labels.index(trial_iter), epoch_iter] = 1\n",
    "        if self.trial_counter % len(self.labels)==0 and epoch_iter == self.epoch_num-1:\n",
    "            self.acc_storage[CV_iter] = np.mean(self.result_matrix[CV_iter,:,:])\n",
    "            print('ACC of the {} cross validation loop is: {}'.format(CV_iter,self.acc_storage[CV_iter]))\n",
    "        \n",
    "    def ACC_calculate(self):\n",
    "        self.overall_ACC = np.mean(self.acc_storage)\n",
    "        print('Overall ACC of current data is {}'.format(self.overall_ACC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_trainer():\n",
    "    \n",
    "    def __init__(self, usrname, data_path, block_num, spatial_filter_type='TRCA', cross_validation=True) -> None:\n",
    "        with open('config.json') as file:\n",
    "            self.data_config = json.load(file)\n",
    "        data_dir = pjoin(data_path,usrname,block_num,'EEG.mat')\n",
    "        self.raw_data = loadmat(data_dir)\n",
    "        self.raw_data = self.raw_data['EEG'][0]\n",
    "        self.event = self.raw_data['event'][0]\n",
    "        self.event_size = self.event.shape[0]\n",
    "        self.data = self.raw_data['data'][0]\n",
    "        # Read experiment paramter setting in json file\n",
    "        self.blocks_in_data = self.data_config['blocks_in_data']\n",
    "        self.epochs_in_data = self.data_config['epochs_in_trials']\n",
    "        self.slice_data_storage = dict()\n",
    "        self.template_storage = dict()\n",
    "        self.sample_rate = self.raw_data['srate'][0][0][0]\n",
    "        self.epoch_length = int(self.data_config['epoch_length'][block_num]*self.sample_rate)\n",
    "        self.visual_delay = int(0.14*self.sample_rate)\n",
    "        self.cross_validation = cross_validation\n",
    "        # Initail time filter paramters here\n",
    "        self.filter_object = filter_applyer(self.sample_rate,7.0,80.0,64,filter_type='FIR')\n",
    "        self.spatial_filter_type = spatial_filter_type\n",
    "        self.template_storage = dict()\n",
    "\n",
    "        self.data_slice()\n",
    "   \n",
    "    def data_slice(self):\n",
    "        for event_iter in range(self.event_size):\n",
    "            event_type = self.event[event_iter][0][0][0]\n",
    "            event_time_stamp = int(self.event[event_iter][0][1][0][0])\n",
    "            epoch_cut = self.data[:,event_time_stamp+self.visual_delay:event_time_stamp+self.visual_delay+self.epoch_length]\n",
    "            #Zero-mean (Must DO!)\n",
    "            epoch_cut = epoch_cut-np.mean(epoch_cut,axis=-1,keepdims=True)\n",
    "            filtered_epoch_cut = self.filter_object.filter_apply(epoch_cut)\n",
    "            event_list = self.slice_data_storage.setdefault(event_type, list())\n",
    "            event_list.append(filtered_epoch_cut)\n",
    "        self.event_series = self.slice_data_storage.keys()\n",
    "        print('Data sliced ready!')\n",
    "        print('Total number of events: {}'.format(len(self.slice_data_storage)))\n",
    "    \n",
    "    def trainer(self):\n",
    "        self.spatial_filters = dict()\n",
    "        for train_trial_iter in self.event_series:\n",
    "            self.spatial_filters[train_trial_iter] = self.feature_extract(self.slice_data_storage[train_trial_iter])\n",
    "            self.template_calculate(self.slice_data_storage[train_trial_iter], train_trial_iter)\n",
    "\n",
    "    def feature_extract(self,data):\n",
    "        if self.spatial_filter_type == 'TRCA':\n",
    "            return trca_matrix(data)\n",
    "        raise Exception('Method not define, you can define it manually!')\n",
    "    \n",
    "    def template_calculate(self, train_data, event_type):\n",
    "        self.template_storage[event_type] = np.mean(train_data, axis=0)\n",
    "        self.template_events = list(self.template_storage.keys())\n",
    "    \n",
    "    def train_result_get(self):\n",
    "        return self.spatial_filters, self.template_storage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_cross_validation(data_trainer):\n",
    "    \n",
    "    def cross_validation_runner(self):\n",
    "        self.dataset_split_index = train_test_split(split_type='K_Fold',test_sample_num=self.epochs_in_data, total_sample_num=self.epochs_in_data*self.blocks_in_data)\n",
    "        self.result_saver = result_analyser(self.event_series,self.dataset_split_index.shape[0],self.epochs_in_data)\n",
    "        for cross_validation_iter in range(self.dataset_split_index.shape[0]):\n",
    "            self.CV_iter = cross_validation_iter\n",
    "            print('cross validation loop: {}'.format(cross_validation_iter))\n",
    "            validation_index = self.dataset_split_index[cross_validation_iter,:]\n",
    "            self.trainer(1-validation_index)\n",
    "            self.tester(validation_index)\n",
    "        self.result_saver.ACC_calculate()\n",
    "    \n",
    "    def trainer(self,select_index):\n",
    "        self.spatial_filters = dict()\n",
    "        for train_trial_iter in self.event_series:\n",
    "            self.spatial_filters[train_trial_iter] = self.feature_extract(np.array(self.slice_data_storage[train_trial_iter])[select_index==1,:,:])\n",
    "            self.template_calculate(np.array(self.slice_data_storage[train_trial_iter])[select_index==1,:,:],train_trial_iter)\n",
    "    \n",
    "    def tester(self,select_index):\n",
    "        self.corrcoef_storage = dict()\n",
    "        for test_trial_iter in self.event_series:\n",
    "            test_epoches = np.array(self.slice_data_storage[test_trial_iter])[select_index==1,:,:]\n",
    "            corrcoef_list = self.corrcoef_storage.setdefault(test_trial_iter, list())\n",
    "            for test_epoch_iter in range(test_epoches.shape[0]):\n",
    "                coef_vector = pattern_match(test_epoches[test_epoch_iter,:,:],self.spatial_filters,self.template_storage)\n",
    "                corrcoef_list.append(coef_vector)\n",
    "                self.result_saver.result_decide(coef_vector,self.CV_iter,test_trial_iter,test_epoch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 125)\n",
      "Data sliced ready!\n",
      "Total number of events: 12\n",
      "cross validation loop: 0\n",
      "ACC of the 0 cross validation loop is: 0.9333333333333333\n",
      "cross validation loop: 1\n",
      "ACC of the 1 cross validation loop is: 0.85\n",
      "cross validation loop: 2\n",
      "ACC of the 2 cross validation loop is: 0.8666666666666667\n",
      "cross validation loop: 3\n",
      "ACC of the 3 cross validation loop is: 0.8666666666666667\n",
      "cross validation loop: 4\n",
      "ACC of the 4 cross validation loop is: 0.9\n",
      "cross validation loop: 5\n",
      "ACC of the 5 cross validation loop is: 0.8\n",
      "Overall ACC of current data is 0.8694444444444445\n"
     ]
    }
   ],
   "source": [
    "# Test cross_validation class\n",
    "tester = data_cross_validation('mengqiangfan','./ThesisData/','block4')\n",
    "tester.cross_validation_runner()\n",
    "\n",
    "# TODO: Add support for IIR filter\n",
    "# TODO: Add LDA module\n",
    "# TODO: Test TDCA"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbad2d039937c9fe990f69a077a800efd89bdf2e37926a364738b268742dce4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('EEGtools': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
